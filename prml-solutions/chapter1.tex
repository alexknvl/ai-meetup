\section*{Chapter 1}
\subsection*{Introduction}
\setcounter{subsection}{1}

% Problem 1.1
\begin{problem}
  \begin{align*}
    y(x, w) &= \sum_i w_i \phi_i(x) \\
    E(w)    &= \frac{1}{2} \sum_j \left(y(x_j, w) - t_j\right)^2
  \end{align*}
  Setting the derivative of $E$ to zero with respect to $w_i$:
  \begin{align*}
    \frac{\partial E}{\partial w_i} &=
      \frac{1}{2} \cdot 2 \sum_j \left(y(x_j, w) - t_j\right) \cdot \phi_i(x_j) \\
      &= \sum_j \phi_i(x_j) y(x_j, w) - \sum_j t_j \phi_i(x_j) \\
      &= \sum_j \phi_i(x_j) \sum_k w_k \phi_k(x_j) - \sum_j t_j \phi_i(x_j) \\
      &= 0
  \end{align*}

  Let's define $A_{ik} = \sum_j \phi_i(x_j) \phi_k(x_j)$ and $T_i = \sum_j t_j \phi_i(x_j)$, then:
  \begin{align*}
    &\sum_j \phi_i(x_j)\sum_k w_k \phi_k(x_j) = \sum_j t_j \phi_i(x_j) \\
    &\sum_k w_k \sum_j \phi_i(x_j) \phi_k(x_j) = T_i \\
    &\sum_k A_{ik} w_k  = T_i
  \end{align*}

  Which is equivalent to a matrix equation $\mathcal{A} w = \mathcal{T}$.
\end{problem}

% Problem 1.2
\begin{problem}
  \begin{align*}
    E(w)    &= \frac{1}{2} \sum_j \left(y(x_j, w) - t_j\right)^2 + \frac{\lambda}{2}\sum_k w_k^2
  \end{align*}
  Setting the derivative of $E$ to zero with respect to $w_i$:
  \begin{align*}
    \frac{\partial E}{\partial w_i} &=
      \sum_k A_{ik} w_k - T_i + \frac{\partial}{\partial w_i}\left[\frac{\lambda}{2}\sum_k w_k^2\right] \\
      &= \sum_k A_{ik} w_k - T_i + \lambda w_i \\
      &= \sum_k A_{ik} w_k - T_i + \lambda \sum_k \delta_{ik} w_k \\
      &= \sum_k (A_{ik} + \delta_{ik}) w_k - T_i \\
      &= \sum_k A^{*}_{ik} w_k - T_i \\
      &= 0
  \end{align*}
\end{problem}

% Problem 1.3
\begin{problem}
  \begin{align*}
    P(F) = \sum_B P(F | B) P(B)
  \end{align*}
  Substituting $F=a$ (apple):
  \begin{align*}
    P(F=a) &= P(F=a | B=r) P(B=r) + P(F=a | B=b) P(B=b) + P(F=a | B=g) P(B=g) \\
           &= \frac{3}{10} \cdot 0.2 + \frac{1}{2} \cdot 0.2 + \frac{3}{10} \cdot 0.6 \\
           &= \frac{6}{100} + \frac{10}{100} + \frac{18}{100} \\
           &= \frac{34}{100} \\
           &= 34\%
  \end{align*}

  WIP
\end{problem}

% Problem 1.4
\begin{problem}
  Assume that $x = g(y)$, a strictly monotonically increasing function.
  Then $dx = g'(y) dy$ and
  \begin{align*}
    p_x(x) dx &= p_y(y) dy \\
              &= p_x(g(y)) g'(y) dy \\
    p_y(y) &= p_x(g(y)) g'(y)
  \end{align*}

  Differentiating with respect to $y$:
  \begin{align*}
    p_y'(y) &= p_x'(g(y)) (g'(y))^2 + p_x(g(y)) g''(y)
  \end{align*}

  If $p_x'(x_0) = 0$ and $x_0 = g(y_0)$,
  \begin{align*}
    p_y'(y_0) &= p_x'(g(y_0)) (g'(y_0))^2 + p_x(g(y_0)) g''(y_0) \\
              &= 0 \cdot (g'(y_0))^2 + p_x(g(y_0)) g''(y_0) \\
              &= p_x(x_0) g''(y_0)
  \end{align*}

  This means that unless $g''(y_0)$ is zero, an extremum of $p_x(x)$ does not
  map to an extremum of $p_y(y)$.

  Is $g''(y_0)=0$ a sufficient condition for a maximum of $p_x(x)$
  to map to a maximum of $p_y(y)$? If $p_x(x)$ reaches a maximum at $x_0$ and
  $p_x(x)$ is sufficiently smooth, then $p_x''(x_0) \leq 0$.
  Let's find $p_y''(y)$:

  \begin{align*}
    p_y'(y) &= p_x'(g(y)) (g'(y))^2 + p_x(g(y)) g''(y) \\
    p_y''(y) &= p_x''(g(y)) (g'(y))^3 + 2 p_x'(g(y)) g'(y) g''(y) +
                p_x'(g(y)) g'(y) g''(y) + p_x(g(y)) g'''(y) \\
             &= p_x''(g(y)) (g'(y))^3 + 3 p_x'(g(y)) g'(y) g''(y) + p_x(g(y)) g'''(y) \\
  \end{align*}

  At $x=x_0, y=y_0$ assuming $g''(y_0)=0$,
  \begin{align*}
    p_y''(y) &= p_x''(x_0) (g'(y))^3 + p_x(g(y)) g'''(y) \\
  \end{align*}

  We can see that $p_x''(x_0) \leq 0$ is not a sufficient condition for $p_y''(y) \leq 0$.
  In fact, if $p_x''(x_0) (g'(y))^3 + p_x(g(y)) g'''(y) > 0$, then a maximum of $p_x(x_0)$
  may transform into a minimum of $p_y(y)$!

  TODO: plot a normal distribution being transformed under $x = g(y) = y^3 + y$.
\end{problem}

% Problem 1.5
\begin{problem}
  Obvious.
\end{problem}

% Problem 1.6
\begin{problem}
  \begin{align*}
    E_{xy}\left[(x-E[x])(y-E[y])\right] \\
    ~~&= \int \int p(x, y) (x-E[x])(y-E[y]) dx dy \\
      &= \int \int p(x) p(y) (x-E[x])(y-E[y]) dx dy \\
      &= \int p(y) \int p(x) (x-E[x])(y-E[y]) dx dy \\
      &= \int p(y) (E[x]-E[x])(y-E[y]) dy \\
      &= (E[x]-E[x])(E[y]-E[y]) \\
  \end{align*}
\end{problem}

% Problem 1.7
\begin{problem}
  Let $I = \int_{-\infty}^{\infty} exp\left(-\frac{x^2}{2\sigma^2}\right) dx$, then
  \begin{align*}
    I^2 &= \int_{-\infty}^{\infty} \int_{-\infty}^{\infty}
      \exp\left(-\frac{x^2}{2\sigma^2}-\frac{y^2}{2\sigma^2}\right) dx dy \\
        &= \int_{0}^{\infty} \exp\left(-\frac{r^2}{2\sigma^2}\right) 2\pi r dr
  \end{align*}
  Substituting $u=r^2$ and $du = 2 r dr$,
  \begin{align*}
    I^2 &= \int_{0}^{\infty} \exp\left(-\frac{u}{2\sigma^2}\right) \pi du \\
        &= -2 \pi \sigma^2 \left( \lim_{x\to\infty} exp\left(-\frac{x^2}{2\sigma^2}\right) -
                                  exp(0) \right) \\
        &= 2 \pi \sigma^2
  \end{align*}

  Therefore $I = \sqrt{2 \pi \sigma^2}$.
\end{problem}

% Problem 1.8
\begin{problem}
  \begin{align*}
    E[x] &= \int_{-\infty}^{\infty} x \cdot \frac{1}{\sqrt{2\pi \sigma^2}}
      exp \left(-\frac{(x-\mu)^2}{2\sigma^2}\right) dx \\
         &\xlongequal[dv = dx]{v = x - \mu}
         \int_{-\infty}^{\infty} v \cdot \frac{1}{\sqrt{2\pi \sigma^2}}
         exp \left(-\frac{v^2}{2\sigma^2}\right) dv + \mu
  \end{align*}

  And since $P(x)$ is symmetric, $\int v P(v) dv = 0$, so $E[x] = \mu$.

  \begin{align*}
    \int_{-\infty}^{\infty} \frac{1}{\sqrt{2\pi \sigma^2}}
      exp \left(-\frac{(x-\mu)^2}{2\sigma^2}\right) dx = 1
  \end{align*}

  Differentiating both sides as functions of $u = \sigma^2$, we get
  \begin{align*}
    \int_{-\infty}^{\infty} \left\{ -\frac{1}{\sigma^2} P(x) +
                                     \frac{(x - \mu)^2}{\sigma^4} P(x)
                            \right\} dx = 0 \\
  \end{align*}

  Both sub-integrals are expectations:
  \begin{align*}
    -E_x\left[ \frac{1}{\sigma^2} \right] +
     E_x\left[ \frac{(x - \mu)^2}{\sigma^4} \right] = 0
  \end{align*}

  Thus
  \begin{align*}
    \sigma^2 = E_x\left[ \frac{(x - \mu)^2}{\sigma^2} \right]
  \end{align*}
\end{problem}

% Problem 1.9
\begin{problem}
  To find the maximum of $N(x | \mu, \sigma^2)$, we have to set
  \begin{align*}
    \frac{\partial}{\partial x} N(x | \mu, \sigma^2) = 0
  \end{align*}

  But it is easier to do first take the log of probability density:
  \begin{align*}
    \frac{\partial}{\partial x} \ln N(x | \mu, \sigma^2) &=
    \frac{\partial}{\partial x} \left[ -\frac{1}{2} \ln (2 \pi \sigma^2) - \frac{(x - \mu)^2}{2 \sigma^2} \right] \\
    &= \frac{\partial}{\partial x} \left[ - (x - \mu)^2 \right] \cdot \frac{1}{2 \sigma^2} \\
    &= -\frac{x - \mu}{\sigma^2} \\
    &= 0
  \end{align*}

  And we immediately get $x = \mu$ and since $\frac{\partial}{\partial x}\ln f(x) = \frac{f'(x)}{f(x)}$,
  \begin{align*}
    \frac{\partial}{\partial x} N(x | \mu, \sigma^2) = -\frac{x - \mu}{\sigma^2} N(x | \mu, \sigma^2)
  \end{align*}

  This allows us to easily calculate the second derivative as well:
  \begin{align*}
    \frac{\partial^2}{\partial^2 x} N(x | \mu, \sigma^2)
        &= -\frac{1}{\sigma^2} N(x | \mu, \sigma^2) \\
        &+ \frac{(x - \mu)^2}{\sigma^4} N(x | \mu, \sigma^2)
  \end{align*}

  Substituting $x = \mu$, we see that the second derivative is $-\sigma^{-2}$,
  a negative number, thus $x = \mu$ is indeed a local (and global) minimum.

  TODO add the N-dimensional case.
\end{problem}

% Problem 1.10
\begin{problem}
  \begin{align*}
    E_{xy}\left[x + z\right] &= \int \int p(x, z) (x + z) dx dz \\
                &= \int \int p(x)p(z) (x + z) dx dz \\
                &= \int p(z) \left( \int p(x) (x + z) dx \right) dz \\
                &= \int p(z) \left( E[x] + z \right) dz \\
                &= E[x]+ E[z]
  \end{align*}

  \begin{align*}
    \int \int p(x, z) (x + z)^2 dx dz
                &= \int \int p(x) p(z) (x + z)^2 dx dz \\
                &= \int \int p(x) p(z) (x^2 + 2 x z + z^2) dx dz \\
                &= \int p(z) \left( \int p(x) (x^2 + 2 x z + z^2) dx \right) dz \\
                &= \int p(z) \left( E[x^2] + 2 E[x] z + z^2 \right) dz \\
                &= E[x^2] + 2 E[x] E[z] + E[z^2]
  \end{align*}

  Since $var[x] = E[x^2] - E^2[x]$,
  \begin{align*}
    var[x + z] &= E[x^2] + 2 E[x] E[z] + E[z^2] - E^2[x + z] \\
                &= E[x^2] + 2 E[x] E[z] + E[z^2] - E[x]^2 - 2 E[x] E[z] - E[z]^2 \\
                &= var[x] + var[z]
  \end{align*}
\end{problem}

% Problem 1.11
\begin{problem}
  Boring.
\end{problem}

% Problem 1.12
\begin{problem}
  If $n=m$, then $E[x_n x_m] = E[x_n^2] = var[x_n] + E^2[x] = \sigma^2 + \mu^2$.
  If $n\neq m$, then $E[x_n x_m] = E[x_n] E[x_m] = \mu^2$.
  Combined, we get $E[x_n x_m] = \mu^2 + \sigma_{nm} \sigma^2$.
\end{problem}

% Problem 1.13
\begin{problem}
  \begin{align*}
    \sigma_1^2 = \frac{1}{N} \sum_n (x_n - \mu)^2 =
      \frac{1}{N} \sum_n (x_n^2 - 2 x_n \mu + \mu^2)
  \end{align*}

  Taking expectation, we get
  \begin{align*}
    E[\sigma_1^2] &= \frac{1}{N} \cdot N (E[x^2] - 2 E[x] \mu + \mu^2) \\
                  &= E[x^2] - 2 \mu \cdot \mu + \mu^2 \\
                  &= var[x] = \sigma^2
  \end{align*}
\end{problem}