\section*{Chapter 1}
\subsection*{Introduction}
\setcounter{subsection}{1}

% Problem 1.1
\begin{problem}
  \begin{align*}
    y(x, w) &= \sum_i w_i \phi_i(x) \\
    E(w)    &= \frac{1}{2} \sum_j \left(y(x_j, w) - t_j\right)^2
  \end{align*}
  Setting the derivative of $E$ to zero with respect to $w_i$:
  \begin{align*}
    \frac{\partial E}{\partial w_i} &=
      \frac{1}{2} \cdot 2 \sum_j \left(y(x_j, w) - t_j\right) \cdot \phi_i(x_j) \\
      &= \sum_j \phi_i(x_j) y(x_j, w) - \sum_j t_j \phi_i(x_j) \\
      &= \sum_j \phi_i(x_j) \sum_k w_k \phi_k(x_j) - \sum_j t_j \phi_i(x_j) \\
      &= 0
  \end{align*}

  Let's define $A_{ik} = \sum_j \phi_i(x_j) \phi_k(x_j)$ and $T_i = \sum_j t_j \phi_i(x_j)$, then:
  \begin{align*}
    &\sum_j \phi_i(x_j)\sum_k w_k \phi_k(x_j) = \sum_j t_j \phi_i(x_j) \\
    &\sum_k w_k \sum_j \phi_i(x_j) \phi_k(x_j) = T_i \\
    &\sum_k A_{ik} w_k  = T_i
  \end{align*}

  Which is equivalent to a matrix equation $\mathcal{A} w = \mathcal{T}$.
\end{problem}

% Problem 1.2
\begin{problem}
  \begin{align*}
    E(w)    &= \frac{1}{2} \sum_j \left(y(x_j, w) - t_j\right)^2 + \frac{\lambda}{2}\sum_k w_k^2
  \end{align*}
  Setting the derivative of $E$ to zero with respect to $w_i$:
  \begin{align*}
    \frac{\partial E}{\partial w_i} &=
      \sum_k A_{ik} w_k - T_i + \frac{\partial}{\partial w_i}\left[\frac{\lambda}{2}\sum_k w_k^2\right] \\
      &= \sum_k A_{ik} w_k - T_i + \lambda w_i \\
      &= \sum_k A_{ik} w_k - T_i + \lambda \sum_k \delta_{ik} w_k \\
      &= \sum_k (A_{ik} + \delta_{ik}) w_k - T_i \\
      &= \sum_k A^{*}_{ik} w_k - T_i \\
      &= 0
  \end{align*}
\end{problem}

% Problem 1.3
\begin{problem}
  \begin{align*}
    P(F) = \sum_B P(F | B) P(B)
  \end{align*}
  Substituting $F=a$ (apple):
  \begin{align*}
    P(F=a) &= P(F=a | B=r) P(B=r) + P(F=a | B=b) P(B=b) + P(F=a | B=g) P(B=g) \\
           &= \frac{3}{10} \cdot 0.2 + \frac{1}{2} \cdot 0.2 + \frac{3}{10} \cdot 0.6 \\
           &= \frac{6}{100} + \frac{10}{100} + \frac{18}{100} \\
           &= \frac{34}{100} \\
           &= 34\%
  \end{align*}

  WIP
\end{problem}

% Problem 1.4
\begin{problem}
  Assume that $x = g(y)$, a strictly monotonically increasing function.
  Then $dx = g'(y) dy$ and
  \begin{align*}
    p_x(x) dx &= p_y(y) dy \\
              &= p_x(g(y)) g'(y) dy \\
    p_y(y) &= p_x(g(y)) g'(y)
  \end{align*}

  Differentiating with respect to $y$:
  \begin{align*}
    p_y'(y) &= p_x'(g(y)) (g'(y))^2 + p_x(g(y)) g''(y)
  \end{align*}

  If $p_x'(x_0) = 0$ and $x_0 = g(y_0)$,
  \begin{align*}
    p_y'(y_0) &= p_x'(g(y_0)) (g'(y_0))^2 + p_x(g(y_0)) g''(y_0) \\
              &= 0 \cdot (g'(y_0))^2 + p_x(g(y_0)) g''(y_0) \\
              &= p_x(x_0) g''(y_0)
  \end{align*}

  This means that unless $g''(y_0)$ is zero, an extremum of $p_x(x)$ does not
  map to an extremum of $p_y(y)$.

  Is $g''(y_0)=0$ a sufficient condition for a maximum of $p_x(x)$
  to map to a maximum of $p_y(y)$? If $p_x(x)$ reaches a maximum at $x_0$ and
  $p_x(x)$ is sufficiently smooth, then $p_x''(x_0) \leq 0$.
  Let's find $p_y''(y)$:

  \begin{align*}
    p_y'(y) &= p_x'(g(y)) (g'(y))^2 + p_x(g(y)) g''(y) \\
    p_y''(y) &= p_x''(g(y)) (g'(y))^3 + 2 p_x'(g(y)) g'(y) g''(y) + p_x'(g(y)) g'(y) g''(y) + p_x(g(y)) g'''(y)
  \end{align*}
\end{problem}